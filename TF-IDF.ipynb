{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import numpy as np\n",
    "from scipy.sparse.csr import csr_matrix "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "Doc_List=[\"This is document1, Connection Broken Error\",\"This is document2, Connection could not be established\",\"This is document3, Test Bed Error\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize the model with the corpus\n",
    "<ol>\n",
    "  <li><b>StopWords</b>:  List of stop words that should be considered according to our context.</li>\n",
    "  <li><b>max_df</b>:  Eliminate words in feature set that occur more than this percentage.</li>\n",
    "  <li><b>min_df</b>:  Eliminate words in feature set that occur less than this percentage.</li>\n",
    "</ol>  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf = TfidfVectorizer(input=text, analyzer='word',max_df=0.8,min_df=0.1, stop_words = 'english')\n",
    "tfidf_matrix =  tf.fit_transform(Doc_List)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<i>0.8 means remove words that occur in more than 80% of documents.</i>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['bed',\n",
       " 'broken',\n",
       " 'connection',\n",
       " 'document1',\n",
       " 'document2',\n",
       " 'document3',\n",
       " 'error',\n",
       " 'established',\n",
       " 'test']"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_names=tf.get_feature_names()\n",
    "feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_features= list()\n",
    "for doc in range(0,len(Doc_List)):\n",
    "    feature_index = tfidf_matrix[doc,:].nonzero()[1]\n",
    "    tfidf_scores = zip(feature_index, [tfidf_matrix[doc, x] for x in feature_index])\n",
    "    term_document_list=list()\n",
    "    for w, s in [(feature_names[i], s) for (i, s) in tfidf_scores]:\n",
    "        term_document_list.append((w, s))\n",
    "    transformed_features.append(term_document_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remaining words In document after transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[('document1', 0.5628290964997665), ('connection', 0.4280460350631185), ('broken', 0.5628290964997665), ('error', 0.4280460350631185)], [('connection', 0.4736296010332684), ('document2', 0.6227660078332259), ('established', 0.6227660078332259)], [('error', 0.4020402441612698), ('document3', 0.5286346066596935), ('test', 0.5286346066596935), ('bed', 0.5286346066596935)]]\n"
     ]
    }
   ],
   "source": [
    "print(transformed_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
